# RAG 系统文档处理完整指南

> **更新日期**: 2025-10-31  
> **系统版本**: v2.0

---

## 快速开始

### 完整工作流程

```
1. 用户上传文档（PDF/Word/TXT）
   ↓
2. 系统解析文档提取文本
   ↓
3. 智能分块（500字/块，50字重叠）
   ↓
4. Ollama 向量化（Qwen3-Embedding-4B）
   ↓
5. 存储到 ChromaDB
   ↓
6. 用户提问
   ↓
7. 问题向量化
   ↓
8. ChromaDB 检索相关文档
   ↓
9. LLM 生成回答（qwen3:4b）
   ↓
10. 返回答案 + 引用来源
```

---

## 功能模块

### 1. 文档处理器 (`utils/document_processor.py`)

**支持格式**:
- ✅ PDF（推荐使用可复制文本的PDF）
- ✅ Word（.doc, .docx）
- ✅ TXT（UTF-8, GBK 编码）

**核心功能**:
- 文档文本提取
- 文本清理（去除特殊字符）
- 智能分块（保持语义完整）
- 元数据管理

### 2. RAG 服务 (`utils/rag_service.py`)

**核心功能**:
- 向量嵌入（Ollama Embedding）
- 文档向量化存储
- 语义检索
- LLM 问答生成

### 3. 知识库 API (`api/knowledge_base.py`)

**核心接口**:
- 文档上传
- 文档列表
- 文档删除
- 知识库管理

---

## 使用示例

### 步骤 1: 创建知识库

```bash
curl -X POST http://localhost:8000/api/knowledge-base/create \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "技术文档库",
    "code": "tech_docs",
    "description": "技术相关文档",
    "visible": "all"
  }'
```

### 步骤 2: 上传文档

```bash
curl -X POST http://localhost:8000/api/knowledge-base/upload-document \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "file=@document.pdf" \
  -F "knowledgeBaseId=kb_xxx"
```

**预期响应**:
```json
{
  "error": 0,
  "message": "文档上传成功",
  "body": {
    "documentId": "doc_xxx",
    "documentName": "document.pdf",
    "chunkCount": 25,
    "fileSize": 1024000
  }
}
```

**后台处理**:
```
[INFO] 文件保存成功: uploads/doc_xxx_document.pdf, 大小: 1024000 字节
[INFO] PDF 解析成功，提取文本长度: 5000 字符
[INFO] 文档分块完成: 25 个块
[INFO] 向知识库 kb_xxx 添加了 25 个文档块
[INFO] 文档上传成功: document.pdf (ID: doc_xxx), 共 25 个块
```

### 步骤 3: 创建对话会话

```bash
curl -X POST http://localhost:8000/api/chat/session/create \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "knowledgeBaseId": "kb_xxx",
    "modelId": "model_xxx"
  }'
```

### 步骤 4: 智能问答

```bash
curl -X POST http://localhost:8000/api/chat/message/send \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "sessionId": "session_xxx",
    "question": "文档中如何实现用户认证？"
  }'
```

**响应示例**:
```json
{
  "error": 0,
  "message": "success",
  "body": {
    "answer": "根据文档内容，用户认证主要通过以下步骤实现：\n1. 用户提交登录凭证\n2. 系统验证用户名和密码\n3. 生成JWT令牌\n4. 返回令牌给客户端...",
    "references": [
      {
        "documentId": "doc_xxx",
        "documentName": "document.pdf",
        "chunkId": "doc_xxx_chunk_5",
        "similarity": 0.92,
        "content": "用户认证模块使用 JWT 令牌进行身份验证..."
      }
    ]
  }
}
```

---

## 配置参数

### 分块参数

```bash
# .env 配置
CHUNK_SIZE=500          # 每块字符数
CHUNK_OVERLAP=50        # 重叠字符数
```

**调优建议**:

| 文档类型 | CHUNK_SIZE | CHUNK_OVERLAP | 说明 |
|---------|------------|---------------|------|
| 技术文档 | 500 | 50 | 平衡精度和召回 |
| 长篇文章 | 800 | 100 | 保持上下文 |
| 问答FAQ | 300 | 30 | 快速定位 |

### 检索参数

```bash
TOP_K=5                 # 检索文档数量
SIMILARITY_THRESHOLD=0.7 # 相似度阈值（0-1）
```

**调优建议**:

| 场景 | TOP_K | SIMILARITY_THRESHOLD |
|------|-------|---------------------|
| 精准问答 | 3 | 0.8 |
| 标准模式 | 5 | 0.7 |
| 广泛检索 | 10 | 0.6 |

---

## 性能指标

### 文档处理速度

| 文档类型 | 大小 | 处理时间 | 块数量 |
|---------|------|---------|--------|
| PDF | 1MB | ~5秒 | ~20块 |
| Word | 500KB | ~3秒 | ~15块 |
| TXT | 200KB | ~1秒 | ~10块 |

### 检索性能

| 知识库大小 | 检索时间 | 向量化时间 |
|-----------|---------|-----------|
| < 100文档 | < 100ms | ~200ms |
| 100-1000文档 | < 500ms | ~200ms |
| > 1000文档 | < 1s | ~200ms |

---

## 故障排查

### 文档上传失败

**问题 1**: "文档内容为空或无法解析"

**原因**:
- PDF 是扫描版（无法提取文字）
- 文档格式损坏
- 文档为空

**解决**:
1. 确认 PDF 可以复制文本
2. 使用 OCR 工具转换扫描版 PDF
3. 检查文档完整性

**问题 2**: "不支持的文件类型"

**原因**: 文件扩展名不在支持列表中

**解决**: 转换为 PDF/Word/TXT 格式

### 检索结果不准确

**问题**: 返回的文档片段不相关

**解决**:
1. 提高 `SIMILARITY_THRESHOLD`（提高精度）
2. 减少 `TOP_K`（只返回最相关的）
3. 优化文档质量（结构清晰）
4. 调整分块大小（更细粒度）

---

## 监控和维护

### 日志监控

```bash
# 查看文档处理日志
tail -f logs/app.log | grep -E '文档|分块|向量'

# 查看检索日志
tail -f logs/app.log | grep -E 'RAG|检索'
```

### 数据备份

```bash
# 备份向量数据
tar -czf chroma_backup_$(date +%Y%m%d).tar.gz chroma_db/

# 备份文档文件
tar -czf uploads_backup_$(date +%Y%m%d).tar.gz uploads/
```

### 定期维护

1. **每周**: 检查日志，清理失败的上传
2. **每月**: 备份 ChromaDB 和上传文件
3. **每季度**: 优化向量数据库性能

---

## API 完整参考

### 知识库管理

```
POST /api/knowledge-base/create         # 创建知识库
POST /api/knowledge-base/list           # 知识库列表
POST /api/knowledge-base/detail         # 知识库详情
POST /api/knowledge-base/update         # 更新知识库
POST /api/knowledge-base/delete         # 删除知识库
```

### 文档管理

```
POST /api/knowledge-base/upload-document    # 上传文档
POST /api/knowledge-base/documents          # 文档列表
POST /api/knowledge-base/delete-document    # 删除文档
```

### 对话管理

```
POST /api/chat/knowledge-bases          # 获取知识库（对话用）
POST /api/chat/models                   # 获取模型列表
POST /api/chat/session/create           # 创建会话
POST /api/chat/message/send             # 发送消息
POST /api/chat/sessions                 # 会话历史
POST /api/chat/session/messages         # 会话消息
```

---

## 技术细节

### 文档分块算法

```python
def split_text(text, chunk_size=500, chunk_overlap=50):
    """
    智能分块策略：
    1. 优先按段落分割（\n\n）
    2. 其次按句子分割（。！？）
    3. 保持块之间有重叠（保证语义连续）
    4. 避免在词中间断开
    """
    separators = ['\n\n', '\n', '。', '！', '？', ';', ',', ' ']
    # ... 实现细节
```

### 向量化流程

```python
# 1. 文本 → 向量
embeddings = ollama.embed(
    model="dengcao/Qwen3-Embedding-4B:Q5_K_M",
    texts=chunks
)

# 2. 存储到 ChromaDB
collection.add(
    documents=chunks,
    embeddings=embeddings,
    metadatas=metadata,
    ids=chunk_ids
)

# 3. 检索
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=TOP_K,
    where={"kb_id": kb_id}  # 可选：按知识库过滤
)
```

---

## 相关文档

- [ChromaDB 集成说明](./ChromaDB集成说明.md)
- [Ollama 配置说明](./Ollama配置说明.md)
- [快速开始指南](./快速开始.md)

---

**系统已就绪，开始使用吧！** 🎉

